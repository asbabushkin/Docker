## Лекция 6. Введение в сети.
  ### Основные сети в докере:
  - none - подключение контейрера этой сети обеспечивает полную изоляцию контейнера от внешней сети.
  - host - режим, обратный none. Сетевая изоляция полностью отключена. Контейнер думает, что он хост.
  - bridge - сеть по умолчанию. Контейнеры, находящиеся в этой сети, могут взаимодействовать между собой. Обеспечивается изоляция от внешней сети. Чтобы достучаться до контейнеров извне, нужно прокидывать порты.
  - кастомные сети - создаются на основе сети bridge. Позволяют настроить нужную конфигурацию изоляции контейнеров. Например, 2 контейнера в одной подсети свободно взаимодействуют между собой, а третий контейнер изолирован во второй подсети. Позволяют обмениваться пакетами между контейнерами, используя имя контейнера, а не ip-адрес, который может меняться (зависит от того, каким по счету поднят контейнер). По сути, это некий аналог службы DNS.

  ### Кастомные сети:
  ![picture](https://github.com/asbabushkin/Docker/blob/main/Lesson6/Custom%20networks.png)

Создадим свою сеть:
```
$ docker network create back_net
```
Проверим, что она появилась в списке сетей:
```
docker network ls

>>> NETWORK ID     NAME       DRIVER    SCOPE
>>> 06ad4dfce37e   back_net   bridge    local
>>> f88a4b27649c   bridge     bridge    local
>>> c8d0022ecf62   host       host      local
>>> 3280bcf44f17   none       null      local
```
Давайте попробуем подключить к ней несколько контейнеров.  
Но сначала посмотрим на IP-адреса самой сети. Для этого создадим еще одну сеть:  
```
docker network create back_net2
```
Проинспектируем созданные сети:  
```
$ docker inspect back_net

>>>  "Config": [
>>>      {
>>>          "Subnet": "172.19.0.0/16",
>>>           "Gateway": "172.19.0.1"
>>>      }
```
Здесь адрес сети ("Subnet") - 172.19.0.0/16, а "Gateway": "172.19.0.1" - адрес шлюза.  

Проинспектируем вторую сеть:  
```
$ docker inspect back_net2

>>>  "Config": [
>>>        {
>>>            "Subnet": "172.20.0.0/16",
>>>            "Gateway": "172.20.0.1"
>>>        }
```
Здесь адрес сети и шлюза уже другие, это 2 разные подсети.
Удалим вторую сеть, она не нужна:
```
$ docker network rm back_net2
```
Создадим и подключим контейнеры к нашей кастомной сети:
```
$ docker run --rm -it --name ubuntu_back_net --net=back_net ubuntu:latest
```
проверим сетевые интерфейсы (на хосте):
```
ifconfig

>>>veth8bb5575: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
>>>        inet6 fe80::308b:fcff:fe6a:2c4a  prefixlen 64  scopeid 0x20<link>
>>>        ether 32:8b:fc:6a:2c:4a  txqueuelen 0  (Ethernet)
>>>        RX packets 0  bytes 0 (0.0 B)
>>>        RX errors 0  dropped 0  overruns 0  frame 0
>>>        TX packets 53  bytes 7370 (7.3 KB)
>>>        TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0

```
Их стало больше, veth8bb5575 соответствует нашему контейнеру.
Снова проинспектируем сеть back_net:
```
$ docker inspect back_net

>>>  "Containers": {
>>>      "ea42f2f16640801675b9d71d39596e89396592d4fed6bd42a6e6e5aeff62cfea": {
>>>          "Name": "ubuntu_back_net",
>>>          "EndpointID": "6424e0c784b54dcbbe1fadb82374239d23c3f2c9abb5edf2e0108673bc634de1",
>>>          "MacAddress": "02:42:ac:13:00:02",
>>>          "IPv4Address": "172.19.0.2/16",
>>>          "IPv6Address": ""
>>>      }
```
Видим, что к ней подключился контейнер ubuntu_back_net с IP-адресом 172.19.0.2  
Создадим второй контейнер и подключим его к стандартной сети bridge (она дефолтная, при создании контейнера ее указывать не нужно):
```
$ docker run --rm -it --name ubuntu_bridge ubuntu:latest
```
Снова проверим сетевые интерфейсы на хосте:  
```
$ifconfig
  
>>>  veth8ce67e8: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500
>>>          inet6 fe80::dc8b:76ff:feae:74e4  prefixlen 64  scopeid 0x20<link>
>>>          ether de:8b:76:ae:74:e4  txqueuelen 0  (Ethernet)
>>>          RX packets 0  bytes 0 (0.0 B)
>>>          RX errors 0  dropped 0  overruns 0  frame 0
>>>          TX packets 53  bytes 7370 (7.3 KB)
>>>          TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0
```
Видим, что появился второй veth - veth8ce67e8.
На хосте проинспектируем сеть bridge:
```
$ docker inspect bridge

>>> "Containers": {
>>>             "e8afb95275c51c17484098f28968666d57d62bf19e7b2822b3e5fee429b1038f": {
>>>                 "Name": "ubuntu_bridge",
>>>                 "EndpointID": "d9e16412998c05a6ee79124989e438193955f229bd86a3b9d6321ee5566f75c9",
>>>                 "MacAddress": "02:42:ac:11:00:02",
>>>                 "IPv4Address": "172.17.0.2/16",
>>>                 "IPv6Address": ""
```
Видим, что появился контейнер ubuntu_bridge на IP-адресе 172.17.0.2
Теперь сделаем пинг из контейнера ubuntu_back_net в контейнер ubuntu_bridge. Для этого перейдем на 
вкладку терминала с соответствующим контейнером (или запустим его), установим обновления и утилиту iputils-ping:  
```
root@ea42f2f16640:/# apt-get update
root@ea42f2f16640:/# apt install iputils-ping
root@ea42f2f16640:/# ping 172.17.0.2

>>> PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.
```
И видим, что пинг не проходит. Давайте искать. На хосте воспользуемся утилитой для перехвата и анализа сетевых пакетов tcpdump:
```
$ sudo tcpdump -i veth8bb5575 // здесь veth контейнера, отправляющего пинг ubuntu_back_net

>>> tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
>>> listening on veth8bb5575, link-type EN10MB (Ethernet), capture size 262144 bytes
>>> 12:41:19.551372 IP 172.19.0.2 > 172.17.0.2: ICMP echo request, id 1, seq 356, length 64
>>> 12:41:20.582551 IP 172.19.0.2 > 172.17.0.2: ICMP echo request, id 1, seq 357, length 64
>>> 12:41:21.614727 IP 172.19.0.2 > 172.17.0.2: ICMP echo request, id 1, seq 358, length 64
```
Видим, что через veth контейнера трафик идет.
Проверим gateway сети back_net:
```
$ sudo tcpdump -i br-06ad4dfce37e

>>> tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
>>> listening on br-06ad4dfce37e, link-type EN10MB (Ethernet), capture size 262144 bytes
>>> 12:52:21.822702 IP 172.19.0.2 > 172.17.0.2: ICMP echo request, id 1, seq 986, length 64
>>> 12:52:22.846145 IP 172.19.0.2 > 172.17.0.2: ICMP echo request, id 1, seq 987, length 64
>>> 12:52:23.877275 IP 172.19.0.2 > 172.17.0.2: ICMP echo request, id 1, seq 988, length 64
```
Пакеты через него проходят.
Далее проверим eth0:  
```
$ sudo tcpdump -i enp0s3 icmp

>>>   tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
>>>   listening on enp0s3, link-type EN10MB (Ethernet), capture size 262144 bytes
```
А вот здесь уже трафика нет. Это демонстрирует изоляцию между сетями: контейнеры, подключенные к одной 
подсети не могут обмениваться данными с контейнерами из другой.  

Теперь продемонстрируем второе преимущество кастомных сетей.
Создадим контейнер с именем back и подключим его к сети back_net, в которой уже работает наш первый контейнер ubuntu_back_net.  
```
$ docker run -d -it --rm --name back --net=back_net ubuntu:latest
```
Проинспектируем сеть back_net:  
```
$ docker inspect back_net

>>>  "Containers": {
>>>              "ea42f2f16640801675b9d71d39596e89396592d4fed6bd42a6e6e5aeff62cfea": {
>>>              "Name": "ubuntu_back_net",
>>>               "EndpointID": "6424e0c784b54dcbbe1fadb82374239d23c3f2c9abb5edf2e0108673bc634de1",
>>>               "MacAddress": "02:42:ac:13:00:02",
>>>               "IPv4Address": "172.19.0.2/16"   
>>>           },
>>>           "f8109945836feedd69eeab7a83dec48c3c86b72eca376d8db4cc64604c8fe072": {
>>>               "Name": "back",
>>>               "EndpointID": "5b56544d92fd6f82a24f4cfb4e232cde48771347f7d9ddb411e68cf21326afa8",
>>>               "MacAddress": "02:42:ac:13:00:03",
>>>               "IPv4Address": "172.19.0.3/16"         
>>>           }
```
Видим, что к ней подключены 2 контейнера: ubuntu_back_net и back.
Попробуем из работающего контейнера ubuntu_back_net достучаться до контейнера back по ip-адресу 172.19.0.3 (предварительно установим в контейнере ubuntu_back_net утилиту iputils-ping):

```
root@ea42f2f16640:/# apt-get update
root@ea42f2f16640:/# apt-get install -y iputils-ping
root@ea42f2f16640:/# ping 172.19.0.3
PING 172.19.0.3 (172.19.0.3) 56(84) bytes of data.
64 bytes from 172.19.0.3: icmp_seq=1 ttl=64 time=36.3 ms
64 bytes from 172.19.0.3: icmp_seq=2 ttl=64 time=0.049 ms
64 bytes from 172.19.0.3: icmp_seq=3 ttl=64 time=0.051 ms
```
Как и следовало ожидать, пинг проходит. А теперь попробуем сделать то же самое, используя имя контейнера вместо его ip-адреса:  
```
root@ea42f2f16640:/# ping back      
PING back (172.19.0.3) 56(84) bytes of data.
64 bytes from back.back_net (172.19.0.3): icmp_seq=1 ttl=64 time=0.246 ms
64 bytes from back.back_net (172.19.0.3): icmp_seq=2 ttl=64 time=0.155 ms
64 bytes from back.back_net (172.19.0.3): icmp_seq=3 ttl=64 time=0.050 ms
```
Получилось! Это очень важная особенность кастомных сетей. Поэтому в таких сетях имена контейнеров очень важны, они играют роль своеобразного домена.
Это очень удобно. Например, при написании конфига нам нужно запустить 10 контейнеров. Если мы укажем для них имена и прокинем их через переменные окружения, то все ок. А если использовать автоматически генерируемое имя или ip-адрес, то нам придется их искать вручную, что очень неудобно.
Супер! Просто очень здорово! Очень здорово! )))


  ### Команды:
  * docker network create <название_сети> - создать сеть  
  * docker network rm <название_сети> - удалить сеть  
  * docker network ls - список сетей  
  * docker run --net=<название_сети> <образ> — подключаем контейнер к сети  
  * docker inspect <название_или_ID_объекта> — получить информацию об объектах докера (контейнер, образ, вольюм, сеть)  
  * docker run --rm -it --name ubuntu_back_net --net=back_net custom_ubuntu - создать контейнер      ubuntu_back_net на основе образа custom_ubuntu и подключить его к сети back_net (создана заранее).  

  ### Дополнительные материалы:
  https://docs.docker.com/engine/network/ документация докера по сетям  
  https://habr.com/ru/post/333874/ статья на хабре  
  https://www.youtube.com/watch?v=bKFMS5C4CG0  
 
